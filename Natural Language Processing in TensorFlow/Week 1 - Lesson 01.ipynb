from tensorflow.keras.preprocessing.text import Tokenizer

sentences = [
    'i love my dog',
    'I, love my cat',
    'You love my dog!'
]

# Tokenizer encodes words into integers with num_words that is the dictionary size 
# i.e.top unique words in a set (in this example 100 is to big for 5 word sentences). 
tokenizer = Tokenizer(num_words = 100)
# tokenizer.fit_on_texts(sentances) - encodes sentances into tokens
tokenizer.fit_on_texts(sentences)
# word_index = tokenizer.word_index - return dictionary containing key value pairs { word : token, .... }
# tokenizer has cast all to lower-case and removed punctuation
word_index = tokenizer.word_index
print(word_index)
