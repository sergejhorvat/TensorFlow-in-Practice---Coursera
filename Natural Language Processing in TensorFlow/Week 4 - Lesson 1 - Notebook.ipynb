{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course 3 - Week 4 - Lesson 1 - Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergejhorvat/TensorFlow-in-Practice---Coursera/blob/master/Natural%20Language%20Processing%20in%20TensorFlow/Week%204%20-%20Lesson%201%20-%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOwsuGQQY9OL",
        "colab_type": "code",
        "outputId": "b5d62881-beef-410e-979a-1f64055501eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.16.4)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (2.0.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.0.post1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: functools32>=3.2.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (3.2.3.post2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (0.7.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (2.3.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: enum34>=1.1.6; python_version < \"3.4\" in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (1.1.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==2.0.0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==2.0.0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==2.0.0) (5.4.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (42.0.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.15.5)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.8.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python2.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.2.5)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (4.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.1 in /usr/local/lib/python2.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.4.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PRnDnCW-Z7qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "e0f87ddb-ba6e-4119-a867-3b142ecedd5c"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "# Single song from traditional Irish music\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# tokenize on corpus\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'replied': 246, 'all': 5, 'forget': 87, 'just': 48, 'being': 254, 'able': 227, 'catchers': 146, 'soon': 53, 'cheeks': 208, 'through': 186, 'go': 200, 'colleen': 230, 'chaneys': 237, 'ground': 81, 'tore': 233, 'jig': 162, 'some': 212, 'nolans': 129, 'cried': 191, 'reel': 161, 'father': 74, 'young': 182, 'acres': 80, 'to': 13, 'under': 234, 'weeks': 30, 'might': 104, 'hall': 141, 'gave': 82, 'town': 66, 'them': 57, 'his': 16, 'around': 177, 'chanters': 259, 'big': 243, 'swore': 198, 'entangled': 261, 'they': 19, 'new': 37, 'ogradys': 131, 'him': 33, 'minute': 106, 'groups': 179, 'gray': 144, 'banished': 158, 'potatoes': 125, 'ned': 223, 'mcgilligan': 120, 'sweetheart': 222, 'fainted': 64, 'kerrigan': 207, 'steps': 38, 'she': 14, 'small': 217, 'athy': 67, 'creature': 190, 'round': 25, 'stretched': 231, 'squeezed': 256, 'declared': 214, 'learn': 174, 'old': 55, 'ten': 79, 'doing': 150, 'tea': 128, 'bacon': 127, 'sure': 101, 'further': 202, 'odaly': 111, 'drop': 218, 'out': 32, 'row': 206, 'jeremy': 69, 'for': 7, 'singing': 148, 'away': 40, 'piper': 252, 'near': 253, 'mccarthy': 184, 'then': 240, 'pipes': 257, 'got': 23, 'fall': 171, 'lashings': 121, 'youll': 90, 'red': 210, 'be': 100, 'we': 157, 'kicked': 248, 'leg': 62, 'youd': 167, 'gathered': 196, 'lanigans': 9, 'punch': 122, 'free': 102, 'water': 136, 'peggy': 119, 'brooks': 172, 'merry': 50, 'murther': 193, 'put': 61, 'strangled': 255, 'come': 88, 'went': 134, 'both': 107, 'accident': 180, 'daughter': 147, 'would': 170, 'nelly': 143, 'cakes': 126, 'of': 8, 'ill': 93, 'ach': 163, 'dublin': 59, 'terrible': 249, 'meelia': 192, 'invitation': 103, 'arrived': 117, 'had': 203, 'taras': 140, 'think': 168, 'courting': 132, 'ladies': 124, 'runctions': 241, 'danced': 58, 'wall': 45, 'sweet': 142, 'ructions': 99, 'powerful': 226, 'nonsense': 159, 'julia': 156, 'one': 68, 'satisfaction': 204, 'ask': 105, 'right': 185, 'relations': 43, 'learning': 36, 'twas': 239, 'table': 235, 'miss': 63, 'your': 95, 'once': 138, 'little': 112, 'rows': 98, 'from': 65, 'glisten': 97, 'her': 27, 'songs': 133, 'introduction': 247, 'hoops': 188, 'there': 28, 'girls': 17, 'three': 29, 'long': 39, 'by': 232, 'hed': 199, 'their': 56, 'bees': 108, 'call': 116, 'too': 219, 'ribbons': 260, 'bellows': 258, 'was': 34, 'listen': 92, 'eyes': 96, 'dolans': 130, 'that': 26, 'painted': 215, 'academy': 173, 'mchugh': 245, 'carmody': 197, 'took': 216, 'but': 91, 'casey': 251, 'boys': 24, 'hadnt': 72, 'midst': 205, 'with': 118, 'he': 21, 'me': 52, 'couples': 178, 'myself': 46, 'made': 76, 'room': 154, 'oh': 238, 'up': 31, 'cask': 109, 'judy': 110, 'terrance': 183, 'morgan': 224, 'were': 11, 'smashed': 236, 'happened': 181, 'didnt': 86, 'called': 194, 'battered': 71, 'and': 1, 'pound': 73, 'give': 115, 'in': 4, 'lads': 213, 'ceiling': 169, 'an': 60, 'twist': 160, 'as': 18, 'hullabaloo': 250, 'at': 12, 'lanigan': 70, 'saw': 228, 'whirligig': 155, 'if': 89, 'again': 22, 'end': 262, 'dancing': 49, 'make': 94, 'when': 44, 'same': 209, 'rat': 145, 'til': 20, 'how': 165, 'phelim': 244, 'rose': 211, 'grand': 83, 'party': 84, 'stepped': 15, 'harp': 137, 'lick': 242, 'nice': 47, 'poor': 189, 'ball': 10, 'kinds': 151, 'brothers': 195, 'farm': 78, 'suppose': 221, 'who': 85, 'polkas': 153, 'much': 220, 'tipped': 51, 'mad': 166, 'plenty': 135, 'nothing': 175, 'friends': 42, 'milliner': 113, 'died': 75, 'man': 77, 'a': 3, 'finnertys': 187, 'mavrone': 164, 'i': 6, 'no': 201, 'spent': 35, 'together': 149, 'wink': 114, 'hearty': 176, 'so': 225, 'time': 54, 'fair': 229, 'the': 2, 'wine': 123, 'nonsensical': 152, 'sounded': 139, 'left': 41}\n",
            "263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soPGVheskaQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\t# tokenize one line, convert to int from word_index\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\t# for everyline of text create n-gram tokenized sequence\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences \n",
        " = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label \n",
        "# xs - everything excelp las value = input_sequences[:,:-1]\n",
        "# labels - last value only = input_sequences[:,-1]\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "# sparse one-hot encoding\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBIow3-3jD9Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "1a4a8f16-5467-4f21-e530-2f0e55099cae"
      },
      "source": [
        "print(\"n_gram_sequence length: \" +   str(len(n_gram_sequence)))\n",
        "print(\"Input sequences shape: \" +   str(input_sequences.shape))\n",
        "print(\"max_sequence_len: \" +   str(max_sequence_len))\n",
        "print(input_sequences[0])\n",
        "print(input_sequences[1])\n",
        "print(input_sequences[2])\n",
        "print(input_sequences[3])\n",
        "print(input_sequences[4])\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "print (\"New poem line ngrams: \")\n",
        "print(input_sequences[7])\n",
        "print(input_sequences[8])\n",
        "print(input_sequences[10])\n",
        "print(input_sequences[11])\n",
        "print(input_sequences[12])\n",
        "print (\"New poem line ngrams: \")\n",
        "print(input_sequences[13])\n",
        "print(input_sequences[14])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_gram_sequence length: 8\n",
            "Input sequences shape: (453, 11)\n",
            "max_sequence_len: 11\n",
            "[0 0 0 0 0 0 0 0 0 4 2]\n",
            "[ 0  0  0  0  0  0  0  0  4  2 66]\n",
            "[ 0  0  0  0  0  0  0  4  2 66  8]\n",
            "[ 0  0  0  0  0  0  4  2 66  8 67]\n",
            "[ 0  0  0  0  0  4  2 66  8 67 68]\n",
            "[ 0  0  0  0  4  2 66  8 67 68 69]\n",
            "[ 0  0  0  4  2 66  8 67 68 69 70]\n",
            "New poem line ngrams: \n",
            "[ 0  0  0  0  0  0  0  0  0 71 40]\n",
            "[ 0  0  0  0  0  0  0  0 71 40 20]\n",
            "[ 0  0  0  0  0  0 71 40 20 21 72]\n",
            "[ 0  0  0  0  0 71 40 20 21 72  3]\n",
            "[ 0  0  0  0 71 40 20 21 72  3 73]\n",
            "New poem line ngrams: \n",
            "[ 0  0  0  0  0  0  0  0  0 16 74]\n",
            "[ 0  0  0  0  0  0  0  0 16 74 75]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJtwVB2NbOAP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "c5e81fa4-ec59-4fbe-c154-7c890c0081d8"
      },
      "source": [
        "# Ig we tahe the fiorst line of the poem: \"\"In the town of Athy one Jeremy Lanigan\" \n",
        "# we get the followint tokens: \n",
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "66\n",
            "8\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49Cv68JOakwv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a072bf9d-9360-4f1e-f21c-5989db1510a8"
      },
      "source": [
        "# we can see that 7th n-gam of first line of poem is encoded like this without the las word that is ys\n",
        "print(xs[6])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  4  2 66  8 67 68 69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY-jwvfgbEF8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "eb5f8181-9a5b-4f81-c70c-5e7faa61a8c9"
      },
      "source": [
        "# the 7th n-gram of first line of poem - the label only - one-hot encoded (token = 70):\n",
        "print(ys[6])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtzlUMYadhKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "f66987db-5cf0-47a2-a970-e766099f4877"
      },
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  4  2 66  8 67 68]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4myRpB1c4Gg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ec2244a1-3f63-4c9c-82e3-388b829cdf83"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'replied': 246, 'all': 5, 'forget': 87, 'just': 48, 'being': 254, 'able': 227, 'catchers': 146, 'soon': 53, 'cheeks': 208, 'through': 186, 'go': 200, 'colleen': 230, 'chaneys': 237, 'ground': 81, 'tore': 233, 'jig': 162, 'some': 212, 'nolans': 129, 'cried': 191, 'reel': 161, 'father': 74, 'young': 182, 'acres': 80, 'to': 13, 'under': 234, 'weeks': 30, 'might': 104, 'hall': 141, 'gave': 82, 'town': 66, 'them': 57, 'his': 16, 'around': 177, 'chanters': 259, 'big': 243, 'swore': 198, 'entangled': 261, 'they': 19, 'new': 37, 'ogradys': 131, 'him': 33, 'minute': 106, 'groups': 179, 'gray': 144, 'banished': 158, 'potatoes': 125, 'ned': 223, 'mcgilligan': 120, 'sweetheart': 222, 'fainted': 64, 'kerrigan': 207, 'steps': 38, 'she': 14, 'small': 217, 'athy': 67, 'creature': 190, 'round': 25, 'stretched': 231, 'squeezed': 256, 'declared': 214, 'learn': 174, 'old': 55, 'ten': 79, 'doing': 150, 'tea': 128, 'bacon': 127, 'sure': 101, 'further': 202, 'odaly': 111, 'drop': 218, 'out': 32, 'row': 206, 'jeremy': 69, 'for': 7, 'singing': 148, 'away': 40, 'piper': 252, 'near': 253, 'mccarthy': 184, 'then': 240, 'pipes': 257, 'got': 23, 'fall': 171, 'lashings': 121, 'youll': 90, 'red': 210, 'be': 100, 'we': 157, 'kicked': 248, 'leg': 62, 'youd': 167, 'gathered': 196, 'lanigans': 9, 'punch': 122, 'free': 102, 'water': 136, 'peggy': 119, 'brooks': 172, 'merry': 50, 'murther': 193, 'put': 61, 'strangled': 255, 'come': 88, 'went': 134, 'both': 107, 'accident': 180, 'daughter': 147, 'would': 170, 'nelly': 143, 'cakes': 126, 'of': 8, 'ill': 93, 'ach': 163, 'dublin': 59, 'terrible': 249, 'meelia': 192, 'invitation': 103, 'arrived': 117, 'had': 203, 'taras': 140, 'think': 168, 'courting': 132, 'ladies': 124, 'runctions': 241, 'danced': 58, 'wall': 45, 'sweet': 142, 'ructions': 99, 'powerful': 226, 'nonsense': 159, 'julia': 156, 'one': 68, 'satisfaction': 204, 'ask': 105, 'right': 185, 'relations': 43, 'learning': 36, 'twas': 239, 'table': 235, 'miss': 63, 'your': 95, 'once': 138, 'little': 112, 'rows': 98, 'from': 65, 'glisten': 97, 'her': 27, 'songs': 133, 'introduction': 247, 'hoops': 188, 'there': 28, 'girls': 17, 'three': 29, 'long': 39, 'by': 232, 'hed': 199, 'their': 56, 'bees': 108, 'call': 116, 'too': 219, 'ribbons': 260, 'bellows': 258, 'was': 34, 'listen': 92, 'eyes': 96, 'dolans': 130, 'that': 26, 'painted': 215, 'academy': 173, 'mchugh': 245, 'carmody': 197, 'took': 216, 'but': 91, 'casey': 251, 'boys': 24, 'hadnt': 72, 'midst': 205, 'with': 118, 'he': 21, 'me': 52, 'couples': 178, 'myself': 46, 'made': 76, 'room': 154, 'oh': 238, 'up': 31, 'cask': 109, 'judy': 110, 'terrance': 183, 'morgan': 224, 'were': 11, 'smashed': 236, 'happened': 181, 'didnt': 86, 'called': 194, 'battered': 71, 'and': 1, 'pound': 73, 'give': 115, 'in': 4, 'lads': 213, 'ceiling': 169, 'an': 60, 'twist': 160, 'as': 18, 'hullabaloo': 250, 'at': 12, 'lanigan': 70, 'saw': 228, 'whirligig': 155, 'if': 89, 'again': 22, 'end': 262, 'dancing': 49, 'make': 94, 'when': 44, 'same': 209, 'rat': 145, 'til': 20, 'how': 165, 'phelim': 244, 'rose': 211, 'grand': 83, 'party': 84, 'stepped': 15, 'harp': 137, 'lick': 242, 'nice': 47, 'poor': 189, 'ball': 10, 'kinds': 151, 'brothers': 195, 'farm': 78, 'suppose': 221, 'who': 85, 'polkas': 153, 'much': 220, 'tipped': 51, 'mad': 166, 'plenty': 135, 'nothing': 175, 'friends': 42, 'milliner': 113, 'died': 75, 'man': 77, 'a': 3, 'finnertys': 187, 'mavrone': 164, 'i': 6, 'no': 201, 'spent': 35, 'together': 149, 'wink': 114, 'hearty': 176, 'so': 225, 'time': 54, 'fair': 229, 'the': 2, 'wine': 123, 'nonsensical': 152, 'sounded': 139, 'left': 41}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9vH8Y59ajYL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c19c3d6-ec1a-4417-e5dc-7dca49d90d73"
      },
      "source": [
        "# \n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(xs, ys, epochs=500, verbose=1)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 453 samples\n",
            "Epoch 1/500\n",
            "453/453 [==============================] - 6s 13ms/sample - loss: 5.5699 - accuracy: 0.0287\n",
            "Epoch 2/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 5.5480 - accuracy: 0.0486\n",
            "Epoch 3/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 5.5027 - accuracy: 0.0508\n",
            "Epoch 4/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 5.3541 - accuracy: 0.0508\n",
            "Epoch 5/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 5.1604 - accuracy: 0.0508\n",
            "Epoch 6/500\n",
            "453/453 [==============================] - 0s 257us/sample - loss: 5.0765 - accuracy: 0.0508\n",
            "Epoch 7/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 5.0388 - accuracy: 0.0464\n",
            "Epoch 8/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 5.0087 - accuracy: 0.0486\n",
            "Epoch 9/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 4.9755 - accuracy: 0.0552\n",
            "Epoch 10/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 4.9427 - accuracy: 0.0574\n",
            "Epoch 11/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 4.9070 - accuracy: 0.0640\n",
            "Epoch 12/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 4.8638 - accuracy: 0.0574\n",
            "Epoch 13/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 4.8133 - accuracy: 0.0596\n",
            "Epoch 14/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 4.7609 - accuracy: 0.0662\n",
            "Epoch 15/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 4.7067 - accuracy: 0.0773\n",
            "Epoch 16/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 4.6457 - accuracy: 0.0839\n",
            "Epoch 17/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 4.5864 - accuracy: 0.0883\n",
            "Epoch 18/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 4.5312 - accuracy: 0.0883\n",
            "Epoch 19/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 4.4726 - accuracy: 0.0949\n",
            "Epoch 20/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 4.4272 - accuracy: 0.1015\n",
            "Epoch 21/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 4.3846 - accuracy: 0.0993\n",
            "Epoch 22/500\n",
            "453/453 [==============================] - 0s 273us/sample - loss: 4.3395 - accuracy: 0.1126\n",
            "Epoch 23/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 4.2916 - accuracy: 0.1060\n",
            "Epoch 24/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 4.2541 - accuracy: 0.1104\n",
            "Epoch 25/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 4.2108 - accuracy: 0.1280\n",
            "Epoch 26/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 4.1750 - accuracy: 0.1192\n",
            "Epoch 27/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 4.1324 - accuracy: 0.1302\n",
            "Epoch 28/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 4.1080 - accuracy: 0.1236\n",
            "Epoch 29/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 4.0642 - accuracy: 0.1391\n",
            "Epoch 30/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 4.0214 - accuracy: 0.1302\n",
            "Epoch 31/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 3.9860 - accuracy: 0.1369\n",
            "Epoch 32/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 3.9417 - accuracy: 0.1457\n",
            "Epoch 33/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 3.8984 - accuracy: 0.1611\n",
            "Epoch 34/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 3.8579 - accuracy: 0.1611\n",
            "Epoch 35/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 3.8156 - accuracy: 0.1722\n",
            "Epoch 36/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 3.7722 - accuracy: 0.1722\n",
            "Epoch 37/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 3.7370 - accuracy: 0.1744\n",
            "Epoch 38/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 3.7000 - accuracy: 0.1788\n",
            "Epoch 39/500\n",
            "453/453 [==============================] - 0s 276us/sample - loss: 3.6531 - accuracy: 0.1810\n",
            "Epoch 40/500\n",
            "453/453 [==============================] - 0s 268us/sample - loss: 3.6065 - accuracy: 0.1854\n",
            "Epoch 41/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 3.5660 - accuracy: 0.1876\n",
            "Epoch 42/500\n",
            "453/453 [==============================] - 0s 278us/sample - loss: 3.5234 - accuracy: 0.1876\n",
            "Epoch 43/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 3.4875 - accuracy: 0.2053\n",
            "Epoch 44/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 3.4683 - accuracy: 0.2163\n",
            "Epoch 45/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 3.4113 - accuracy: 0.2340\n",
            "Epoch 46/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 3.3633 - accuracy: 0.2472\n",
            "Epoch 47/500\n",
            "453/453 [==============================] - 0s 277us/sample - loss: 3.3182 - accuracy: 0.2671\n",
            "Epoch 48/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 3.2799 - accuracy: 0.2693\n",
            "Epoch 49/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 3.2488 - accuracy: 0.2936\n",
            "Epoch 50/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 3.2093 - accuracy: 0.2958\n",
            "Epoch 51/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 3.1646 - accuracy: 0.2980\n",
            "Epoch 52/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 3.1325 - accuracy: 0.2870\n",
            "Epoch 53/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 3.0877 - accuracy: 0.3311\n",
            "Epoch 54/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 3.0435 - accuracy: 0.3554\n",
            "Epoch 55/500\n",
            "453/453 [==============================] - 0s 272us/sample - loss: 3.0086 - accuracy: 0.3444\n",
            "Epoch 56/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 2.9615 - accuracy: 0.3819\n",
            "Epoch 57/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 2.9283 - accuracy: 0.3642\n",
            "Epoch 58/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 2.8896 - accuracy: 0.3951\n",
            "Epoch 59/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 2.8564 - accuracy: 0.4040\n",
            "Epoch 60/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 2.8126 - accuracy: 0.4238\n",
            "Epoch 61/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 2.7767 - accuracy: 0.4216\n",
            "Epoch 62/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 2.7455 - accuracy: 0.4437\n",
            "Epoch 63/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 2.7200 - accuracy: 0.4636\n",
            "Epoch 64/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 2.6902 - accuracy: 0.4768\n",
            "Epoch 65/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 2.6549 - accuracy: 0.4680\n",
            "Epoch 66/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 2.6294 - accuracy: 0.4658\n",
            "Epoch 67/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 2.5859 - accuracy: 0.5033\n",
            "Epoch 68/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 2.5486 - accuracy: 0.5166\n",
            "Epoch 69/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 2.5044 - accuracy: 0.5320\n",
            "Epoch 70/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 2.4655 - accuracy: 0.5497\n",
            "Epoch 71/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 2.4380 - accuracy: 0.5651\n",
            "Epoch 72/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 2.4284 - accuracy: 0.5695\n",
            "Epoch 73/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 2.4002 - accuracy: 0.5740\n",
            "Epoch 74/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 2.3641 - accuracy: 0.5806\n",
            "Epoch 75/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 2.3225 - accuracy: 0.5982\n",
            "Epoch 76/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 2.2930 - accuracy: 0.6049\n",
            "Epoch 77/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 2.2725 - accuracy: 0.6181\n",
            "Epoch 78/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 2.2333 - accuracy: 0.6181\n",
            "Epoch 79/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 2.2104 - accuracy: 0.6291\n",
            "Epoch 80/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 2.1865 - accuracy: 0.6402\n",
            "Epoch 81/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 2.1552 - accuracy: 0.6336\n",
            "Epoch 82/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 2.1148 - accuracy: 0.6424\n",
            "Epoch 83/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 2.0865 - accuracy: 0.6578\n",
            "Epoch 84/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 2.0622 - accuracy: 0.6490\n",
            "Epoch 85/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 2.0415 - accuracy: 0.6689\n",
            "Epoch 86/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 2.0137 - accuracy: 0.6578\n",
            "Epoch 87/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 1.9862 - accuracy: 0.6556\n",
            "Epoch 88/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 1.9651 - accuracy: 0.6777\n",
            "Epoch 89/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.9429 - accuracy: 0.6799\n",
            "Epoch 90/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 1.9201 - accuracy: 0.6887\n",
            "Epoch 91/500\n",
            "453/453 [==============================] - 0s 221us/sample - loss: 1.8977 - accuracy: 0.6843\n",
            "Epoch 92/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.8745 - accuracy: 0.6976\n",
            "Epoch 93/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 1.8449 - accuracy: 0.6998\n",
            "Epoch 94/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 1.8399 - accuracy: 0.6998\n",
            "Epoch 95/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 1.8120 - accuracy: 0.7174\n",
            "Epoch 96/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 1.7851 - accuracy: 0.7108\n",
            "Epoch 97/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 1.7596 - accuracy: 0.7174\n",
            "Epoch 98/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 1.7329 - accuracy: 0.7285\n",
            "Epoch 99/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 1.7140 - accuracy: 0.7329\n",
            "Epoch 100/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 1.6930 - accuracy: 0.7395\n",
            "Epoch 101/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 1.6724 - accuracy: 0.7395\n",
            "Epoch 102/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 1.6550 - accuracy: 0.7506\n",
            "Epoch 103/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 1.6533 - accuracy: 0.7506\n",
            "Epoch 104/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 1.6485 - accuracy: 0.7461\n",
            "Epoch 105/500\n",
            "453/453 [==============================] - 0s 257us/sample - loss: 1.6217 - accuracy: 0.7550\n",
            "Epoch 106/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 1.6309 - accuracy: 0.7351\n",
            "Epoch 107/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 1.5991 - accuracy: 0.7572\n",
            "Epoch 108/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 1.5621 - accuracy: 0.7726\n",
            "Epoch 109/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 1.5477 - accuracy: 0.7506\n",
            "Epoch 110/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 1.5269 - accuracy: 0.7594\n",
            "Epoch 111/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 1.5128 - accuracy: 0.7660\n",
            "Epoch 112/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 1.4893 - accuracy: 0.7660\n",
            "Epoch 113/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 1.4627 - accuracy: 0.7682\n",
            "Epoch 114/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 1.4425 - accuracy: 0.7726\n",
            "Epoch 115/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 1.4253 - accuracy: 0.7770\n",
            "Epoch 116/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 1.4064 - accuracy: 0.7925\n",
            "Epoch 117/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 1.3951 - accuracy: 0.7837\n",
            "Epoch 118/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 1.3738 - accuracy: 0.7903\n",
            "Epoch 119/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 1.3559 - accuracy: 0.7969\n",
            "Epoch 120/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 1.3395 - accuracy: 0.7947\n",
            "Epoch 121/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 1.3287 - accuracy: 0.8035\n",
            "Epoch 122/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 1.3088 - accuracy: 0.8168\n",
            "Epoch 123/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 1.3051 - accuracy: 0.8079\n",
            "Epoch 124/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 1.2852 - accuracy: 0.8168\n",
            "Epoch 125/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 1.2701 - accuracy: 0.8234\n",
            "Epoch 126/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 1.2610 - accuracy: 0.8256\n",
            "Epoch 127/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.2378 - accuracy: 0.8322\n",
            "Epoch 128/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 1.2253 - accuracy: 0.8300\n",
            "Epoch 129/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 1.2102 - accuracy: 0.8278\n",
            "Epoch 130/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 1.1998 - accuracy: 0.8300\n",
            "Epoch 131/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 1.1866 - accuracy: 0.8366\n",
            "Epoch 132/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 1.1720 - accuracy: 0.8322\n",
            "Epoch 133/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 1.1628 - accuracy: 0.8389\n",
            "Epoch 134/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 1.1487 - accuracy: 0.8411\n",
            "Epoch 135/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 1.1319 - accuracy: 0.8455\n",
            "Epoch 136/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 1.1187 - accuracy: 0.8477\n",
            "Epoch 137/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 1.1097 - accuracy: 0.8477\n",
            "Epoch 138/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 1.1016 - accuracy: 0.8455\n",
            "Epoch 139/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 1.0914 - accuracy: 0.8433\n",
            "Epoch 140/500\n",
            "453/453 [==============================] - 0s 269us/sample - loss: 1.0975 - accuracy: 0.8389\n",
            "Epoch 141/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 1.0894 - accuracy: 0.8411\n",
            "Epoch 142/500\n",
            "453/453 [==============================] - 0s 259us/sample - loss: 1.0830 - accuracy: 0.8433\n",
            "Epoch 143/500\n",
            "453/453 [==============================] - 0s 265us/sample - loss: 1.1135 - accuracy: 0.8344\n",
            "Epoch 144/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 1.1112 - accuracy: 0.8300\n",
            "Epoch 145/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 1.0841 - accuracy: 0.8389\n",
            "Epoch 146/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 1.0710 - accuracy: 0.8433\n",
            "Epoch 147/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 1.0590 - accuracy: 0.8389\n",
            "Epoch 148/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 1.0297 - accuracy: 0.8587\n",
            "Epoch 149/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 1.0013 - accuracy: 0.8609\n",
            "Epoch 150/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.9859 - accuracy: 0.8631\n",
            "Epoch 151/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.9685 - accuracy: 0.8653\n",
            "Epoch 152/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.9560 - accuracy: 0.8631\n",
            "Epoch 153/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.9447 - accuracy: 0.8609\n",
            "Epoch 154/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.9299 - accuracy: 0.8675\n",
            "Epoch 155/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.9222 - accuracy: 0.8653\n",
            "Epoch 156/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.9105 - accuracy: 0.8675\n",
            "Epoch 157/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.9019 - accuracy: 0.8808\n",
            "Epoch 158/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.8921 - accuracy: 0.8764\n",
            "Epoch 159/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 0.8846 - accuracy: 0.8764\n",
            "Epoch 160/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 0.8740 - accuracy: 0.8808\n",
            "Epoch 161/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.8647 - accuracy: 0.8786\n",
            "Epoch 162/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.8538 - accuracy: 0.8786\n",
            "Epoch 163/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.8451 - accuracy: 0.8786\n",
            "Epoch 164/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.8355 - accuracy: 0.8830\n",
            "Epoch 165/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.8296 - accuracy: 0.8764\n",
            "Epoch 166/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.8229 - accuracy: 0.8764\n",
            "Epoch 167/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.8217 - accuracy: 0.8720\n",
            "Epoch 168/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.8164 - accuracy: 0.8764\n",
            "Epoch 169/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 0.8127 - accuracy: 0.8830\n",
            "Epoch 170/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.8011 - accuracy: 0.8940\n",
            "Epoch 171/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.8403 - accuracy: 0.8764\n",
            "Epoch 172/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.8169 - accuracy: 0.8830\n",
            "Epoch 173/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.8095 - accuracy: 0.8742\n",
            "Epoch 174/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.7910 - accuracy: 0.8808\n",
            "Epoch 175/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.7870 - accuracy: 0.8808\n",
            "Epoch 176/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.7758 - accuracy: 0.8852\n",
            "Epoch 177/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.7629 - accuracy: 0.8962\n",
            "Epoch 178/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.7495 - accuracy: 0.8962\n",
            "Epoch 179/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.7395 - accuracy: 0.9007\n",
            "Epoch 180/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.7295 - accuracy: 0.9029\n",
            "Epoch 181/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.7229 - accuracy: 0.9029\n",
            "Epoch 182/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.7173 - accuracy: 0.8985\n",
            "Epoch 183/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.7062 - accuracy: 0.9007\n",
            "Epoch 184/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.6968 - accuracy: 0.8962\n",
            "Epoch 185/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.6889 - accuracy: 0.9095\n",
            "Epoch 186/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.6804 - accuracy: 0.9073\n",
            "Epoch 187/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.6741 - accuracy: 0.9117\n",
            "Epoch 188/500\n",
            "453/453 [==============================] - 0s 257us/sample - loss: 0.6688 - accuracy: 0.9095\n",
            "Epoch 189/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.6610 - accuracy: 0.9161\n",
            "Epoch 190/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.6564 - accuracy: 0.9139\n",
            "Epoch 191/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.6509 - accuracy: 0.9161\n",
            "Epoch 192/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.6460 - accuracy: 0.9161\n",
            "Epoch 193/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.6424 - accuracy: 0.9117\n",
            "Epoch 194/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.6431 - accuracy: 0.9139\n",
            "Epoch 195/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.6377 - accuracy: 0.9139\n",
            "Epoch 196/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.6287 - accuracy: 0.9139\n",
            "Epoch 197/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.6209 - accuracy: 0.9117\n",
            "Epoch 198/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.6204 - accuracy: 0.9117\n",
            "Epoch 199/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.6133 - accuracy: 0.9183\n",
            "Epoch 200/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.6104 - accuracy: 0.9183\n",
            "Epoch 201/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.6004 - accuracy: 0.9161\n",
            "Epoch 202/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.5932 - accuracy: 0.9183\n",
            "Epoch 203/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.5867 - accuracy: 0.9205\n",
            "Epoch 204/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.5819 - accuracy: 0.9183\n",
            "Epoch 205/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.5729 - accuracy: 0.9205\n",
            "Epoch 206/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 0.5687 - accuracy: 0.9183\n",
            "Epoch 207/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.5680 - accuracy: 0.9183\n",
            "Epoch 208/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.5643 - accuracy: 0.9161\n",
            "Epoch 209/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.5574 - accuracy: 0.9249\n",
            "Epoch 210/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.5519 - accuracy: 0.9205\n",
            "Epoch 211/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.5468 - accuracy: 0.9249\n",
            "Epoch 212/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.5407 - accuracy: 0.9249\n",
            "Epoch 213/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.5380 - accuracy: 0.9227\n",
            "Epoch 214/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.5319 - accuracy: 0.9249\n",
            "Epoch 215/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.5268 - accuracy: 0.9227\n",
            "Epoch 216/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.5222 - accuracy: 0.9249\n",
            "Epoch 217/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.5172 - accuracy: 0.9294\n",
            "Epoch 218/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.5139 - accuracy: 0.9205\n",
            "Epoch 219/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.5069 - accuracy: 0.9272\n",
            "Epoch 220/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.5035 - accuracy: 0.9227\n",
            "Epoch 221/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.4995 - accuracy: 0.9272\n",
            "Epoch 222/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.4963 - accuracy: 0.9249\n",
            "Epoch 223/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.4921 - accuracy: 0.9272\n",
            "Epoch 224/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.4874 - accuracy: 0.9294\n",
            "Epoch 225/500\n",
            "453/453 [==============================] - 0s 270us/sample - loss: 0.4827 - accuracy: 0.9294\n",
            "Epoch 226/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.4778 - accuracy: 0.9316\n",
            "Epoch 227/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.4726 - accuracy: 0.9338\n",
            "Epoch 228/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.4690 - accuracy: 0.9360\n",
            "Epoch 229/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.4640 - accuracy: 0.9316\n",
            "Epoch 230/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.4601 - accuracy: 0.9382\n",
            "Epoch 231/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.4564 - accuracy: 0.9316\n",
            "Epoch 232/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.4541 - accuracy: 0.9360\n",
            "Epoch 233/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.4539 - accuracy: 0.9360\n",
            "Epoch 234/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.4530 - accuracy: 0.9338\n",
            "Epoch 235/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.4491 - accuracy: 0.9382\n",
            "Epoch 236/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.4480 - accuracy: 0.9294\n",
            "Epoch 237/500\n",
            "453/453 [==============================] - 0s 262us/sample - loss: 0.4453 - accuracy: 0.9338\n",
            "Epoch 238/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.4444 - accuracy: 0.9382\n",
            "Epoch 239/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.4455 - accuracy: 0.9316\n",
            "Epoch 240/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.4530 - accuracy: 0.9272\n",
            "Epoch 241/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.4457 - accuracy: 0.9338\n",
            "Epoch 242/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.4519 - accuracy: 0.9316\n",
            "Epoch 243/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.4368 - accuracy: 0.9382\n",
            "Epoch 244/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.4267 - accuracy: 0.9382\n",
            "Epoch 245/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.4194 - accuracy: 0.9360\n",
            "Epoch 246/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.4329 - accuracy: 0.9360\n",
            "Epoch 247/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.4181 - accuracy: 0.9404\n",
            "Epoch 248/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.4116 - accuracy: 0.9382\n",
            "Epoch 249/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.4077 - accuracy: 0.9382\n",
            "Epoch 250/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.4027 - accuracy: 0.9404\n",
            "Epoch 251/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.3996 - accuracy: 0.9426\n",
            "Epoch 252/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.3954 - accuracy: 0.9404\n",
            "Epoch 253/500\n",
            "453/453 [==============================] - 0s 259us/sample - loss: 0.3903 - accuracy: 0.9426\n",
            "Epoch 254/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.3881 - accuracy: 0.9426\n",
            "Epoch 255/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.3850 - accuracy: 0.9426\n",
            "Epoch 256/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.3819 - accuracy: 0.9448\n",
            "Epoch 257/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.3768 - accuracy: 0.9448\n",
            "Epoch 258/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.3738 - accuracy: 0.9426\n",
            "Epoch 259/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.3741 - accuracy: 0.9404\n",
            "Epoch 260/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.3719 - accuracy: 0.9426\n",
            "Epoch 261/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.3670 - accuracy: 0.9426\n",
            "Epoch 262/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 0.3654 - accuracy: 0.9448\n",
            "Epoch 263/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.3606 - accuracy: 0.9426\n",
            "Epoch 264/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.3649 - accuracy: 0.9426\n",
            "Epoch 265/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.3584 - accuracy: 0.9448\n",
            "Epoch 266/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.3530 - accuracy: 0.9448\n",
            "Epoch 267/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.3511 - accuracy: 0.9470\n",
            "Epoch 268/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.3521 - accuracy: 0.9404\n",
            "Epoch 269/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.3722 - accuracy: 0.9316\n",
            "Epoch 270/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.3981 - accuracy: 0.9272\n",
            "Epoch 271/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.3962 - accuracy: 0.9294\n",
            "Epoch 272/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.3670 - accuracy: 0.9426\n",
            "Epoch 273/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.3718 - accuracy: 0.9360\n",
            "Epoch 274/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.3579 - accuracy: 0.9404\n",
            "Epoch 275/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.3489 - accuracy: 0.9448\n",
            "Epoch 276/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.3447 - accuracy: 0.9426\n",
            "Epoch 277/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.3402 - accuracy: 0.9404\n",
            "Epoch 278/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.3456 - accuracy: 0.9382\n",
            "Epoch 279/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.3397 - accuracy: 0.9448\n",
            "Epoch 280/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.3304 - accuracy: 0.9426\n",
            "Epoch 281/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.3251 - accuracy: 0.9426\n",
            "Epoch 282/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.3210 - accuracy: 0.9448\n",
            "Epoch 283/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.3185 - accuracy: 0.9426\n",
            "Epoch 284/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.3152 - accuracy: 0.9404\n",
            "Epoch 285/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.3123 - accuracy: 0.9404\n",
            "Epoch 286/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.3120 - accuracy: 0.9448\n",
            "Epoch 287/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.3121 - accuracy: 0.9426\n",
            "Epoch 288/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.3108 - accuracy: 0.9448\n",
            "Epoch 289/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.3091 - accuracy: 0.9448\n",
            "Epoch 290/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.3054 - accuracy: 0.9426\n",
            "Epoch 291/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.3008 - accuracy: 0.9404\n",
            "Epoch 292/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.3032 - accuracy: 0.9426\n",
            "Epoch 293/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.3008 - accuracy: 0.9426\n",
            "Epoch 294/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.2984 - accuracy: 0.9404\n",
            "Epoch 295/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2951 - accuracy: 0.9448\n",
            "Epoch 296/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.2921 - accuracy: 0.9426\n",
            "Epoch 297/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.2897 - accuracy: 0.9404\n",
            "Epoch 298/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2887 - accuracy: 0.9404\n",
            "Epoch 299/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.2918 - accuracy: 0.9426\n",
            "Epoch 300/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.3226 - accuracy: 0.9316\n",
            "Epoch 301/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.3194 - accuracy: 0.9338\n",
            "Epoch 302/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.3077 - accuracy: 0.9404\n",
            "Epoch 303/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.3000 - accuracy: 0.9426\n",
            "Epoch 304/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.2963 - accuracy: 0.9382\n",
            "Epoch 305/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.2906 - accuracy: 0.9382\n",
            "Epoch 306/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.2816 - accuracy: 0.9426\n",
            "Epoch 307/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.2772 - accuracy: 0.9426\n",
            "Epoch 308/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 0.2743 - accuracy: 0.9382\n",
            "Epoch 309/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.2733 - accuracy: 0.9448\n",
            "Epoch 310/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.2722 - accuracy: 0.9448\n",
            "Epoch 311/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.2679 - accuracy: 0.9426\n",
            "Epoch 312/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.2664 - accuracy: 0.9426\n",
            "Epoch 313/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.2646 - accuracy: 0.9426\n",
            "Epoch 314/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.2631 - accuracy: 0.9404\n",
            "Epoch 315/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.2612 - accuracy: 0.9448\n",
            "Epoch 316/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.2590 - accuracy: 0.9448\n",
            "Epoch 317/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.2578 - accuracy: 0.9426\n",
            "Epoch 318/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2577 - accuracy: 0.9404\n",
            "Epoch 319/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.2582 - accuracy: 0.9426\n",
            "Epoch 320/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.2551 - accuracy: 0.9426\n",
            "Epoch 321/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.2536 - accuracy: 0.9448\n",
            "Epoch 322/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.2524 - accuracy: 0.9426\n",
            "Epoch 323/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.2499 - accuracy: 0.9448\n",
            "Epoch 324/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2486 - accuracy: 0.9448\n",
            "Epoch 325/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 0.2474 - accuracy: 0.9426\n",
            "Epoch 326/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2454 - accuracy: 0.9448\n",
            "Epoch 327/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.2438 - accuracy: 0.9448\n",
            "Epoch 328/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.2427 - accuracy: 0.9426\n",
            "Epoch 329/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.2414 - accuracy: 0.9426\n",
            "Epoch 330/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.2414 - accuracy: 0.9426\n",
            "Epoch 331/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.2397 - accuracy: 0.9448\n",
            "Epoch 332/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.2379 - accuracy: 0.9426\n",
            "Epoch 333/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.2376 - accuracy: 0.9404\n",
            "Epoch 334/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.2370 - accuracy: 0.9426\n",
            "Epoch 335/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 0.2364 - accuracy: 0.9426\n",
            "Epoch 336/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2352 - accuracy: 0.9382\n",
            "Epoch 337/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.2331 - accuracy: 0.9404\n",
            "Epoch 338/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.2330 - accuracy: 0.9426\n",
            "Epoch 339/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2328 - accuracy: 0.9404\n",
            "Epoch 340/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.2314 - accuracy: 0.9404\n",
            "Epoch 341/500\n",
            "453/453 [==============================] - 0s 223us/sample - loss: 0.2290 - accuracy: 0.9404\n",
            "Epoch 342/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.2280 - accuracy: 0.9426\n",
            "Epoch 343/500\n",
            "453/453 [==============================] - 0s 229us/sample - loss: 0.2270 - accuracy: 0.9426\n",
            "Epoch 344/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.2254 - accuracy: 0.9382\n",
            "Epoch 345/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.2253 - accuracy: 0.9426\n",
            "Epoch 346/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.2255 - accuracy: 0.9360\n",
            "Epoch 347/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.2245 - accuracy: 0.9382\n",
            "Epoch 348/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2257 - accuracy: 0.9404\n",
            "Epoch 349/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 0.2216 - accuracy: 0.9448\n",
            "Epoch 350/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.2219 - accuracy: 0.9448\n",
            "Epoch 351/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.2183 - accuracy: 0.9448\n",
            "Epoch 352/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.2175 - accuracy: 0.9448\n",
            "Epoch 353/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.2156 - accuracy: 0.9426\n",
            "Epoch 354/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.2157 - accuracy: 0.9404\n",
            "Epoch 355/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.2147 - accuracy: 0.9404\n",
            "Epoch 356/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.2146 - accuracy: 0.9426\n",
            "Epoch 357/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2131 - accuracy: 0.9426\n",
            "Epoch 358/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.2114 - accuracy: 0.9448\n",
            "Epoch 359/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.2102 - accuracy: 0.9448\n",
            "Epoch 360/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.2082 - accuracy: 0.9426\n",
            "Epoch 361/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.2077 - accuracy: 0.9448\n",
            "Epoch 362/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2066 - accuracy: 0.9448\n",
            "Epoch 363/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.2050 - accuracy: 0.9448\n",
            "Epoch 364/500\n",
            "453/453 [==============================] - 0s 259us/sample - loss: 0.2042 - accuracy: 0.9426\n",
            "Epoch 365/500\n",
            "453/453 [==============================] - 0s 255us/sample - loss: 0.2039 - accuracy: 0.9448\n",
            "Epoch 366/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.2017 - accuracy: 0.9426\n",
            "Epoch 367/500\n",
            "453/453 [==============================] - 0s 232us/sample - loss: 0.2011 - accuracy: 0.9426\n",
            "Epoch 368/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2057 - accuracy: 0.9382\n",
            "Epoch 369/500\n",
            "453/453 [==============================] - 0s 257us/sample - loss: 0.2059 - accuracy: 0.9426\n",
            "Epoch 370/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2142 - accuracy: 0.9360\n",
            "Epoch 371/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.2103 - accuracy: 0.9404\n",
            "Epoch 372/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.2166 - accuracy: 0.9426\n",
            "Epoch 373/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.2178 - accuracy: 0.9470\n",
            "Epoch 374/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2100 - accuracy: 0.9448\n",
            "Epoch 375/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.2048 - accuracy: 0.9404\n",
            "Epoch 376/500\n",
            "453/453 [==============================] - 0s 257us/sample - loss: 0.2098 - accuracy: 0.9426\n",
            "Epoch 377/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 0.2247 - accuracy: 0.9382\n",
            "Epoch 378/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.2217 - accuracy: 0.9360\n",
            "Epoch 379/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.2246 - accuracy: 0.9382\n",
            "Epoch 380/500\n",
            "453/453 [==============================] - 0s 233us/sample - loss: 0.2452 - accuracy: 0.9360\n",
            "Epoch 381/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.2782 - accuracy: 0.9294\n",
            "Epoch 382/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.2516 - accuracy: 0.9360\n",
            "Epoch 383/500\n",
            "453/453 [==============================] - 0s 297us/sample - loss: 0.2289 - accuracy: 0.9338\n",
            "Epoch 384/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.2210 - accuracy: 0.9360\n",
            "Epoch 385/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.2278 - accuracy: 0.9382\n",
            "Epoch 386/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.2303 - accuracy: 0.9316\n",
            "Epoch 387/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.2121 - accuracy: 0.9382\n",
            "Epoch 388/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.2017 - accuracy: 0.9426\n",
            "Epoch 389/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 0.1980 - accuracy: 0.9448\n",
            "Epoch 390/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1950 - accuracy: 0.9470\n",
            "Epoch 391/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.1924 - accuracy: 0.9492\n",
            "Epoch 392/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.1900 - accuracy: 0.9426\n",
            "Epoch 393/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1881 - accuracy: 0.9448\n",
            "Epoch 394/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1879 - accuracy: 0.9448\n",
            "Epoch 395/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.1882 - accuracy: 0.9426\n",
            "Epoch 396/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1861 - accuracy: 0.9404\n",
            "Epoch 397/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1846 - accuracy: 0.9470\n",
            "Epoch 398/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.1839 - accuracy: 0.9404\n",
            "Epoch 399/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1820 - accuracy: 0.9404\n",
            "Epoch 400/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.1814 - accuracy: 0.9448\n",
            "Epoch 401/500\n",
            "453/453 [==============================] - 0s 251us/sample - loss: 0.1794 - accuracy: 0.9470\n",
            "Epoch 402/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1787 - accuracy: 0.9426\n",
            "Epoch 403/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.1778 - accuracy: 0.9448\n",
            "Epoch 404/500\n",
            "453/453 [==============================] - 0s 266us/sample - loss: 0.1771 - accuracy: 0.9448\n",
            "Epoch 405/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1759 - accuracy: 0.9470\n",
            "Epoch 406/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1758 - accuracy: 0.9448\n",
            "Epoch 407/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.1753 - accuracy: 0.9448\n",
            "Epoch 408/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1746 - accuracy: 0.9426\n",
            "Epoch 409/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.1730 - accuracy: 0.9492\n",
            "Epoch 410/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.1724 - accuracy: 0.9470\n",
            "Epoch 411/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1715 - accuracy: 0.9426\n",
            "Epoch 412/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1712 - accuracy: 0.9470\n",
            "Epoch 413/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1702 - accuracy: 0.9448\n",
            "Epoch 414/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1695 - accuracy: 0.9470\n",
            "Epoch 415/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1692 - accuracy: 0.9470\n",
            "Epoch 416/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1681 - accuracy: 0.9448\n",
            "Epoch 417/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1682 - accuracy: 0.9448\n",
            "Epoch 418/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.1676 - accuracy: 0.9448\n",
            "Epoch 419/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1673 - accuracy: 0.9470\n",
            "Epoch 420/500\n",
            "453/453 [==============================] - 0s 254us/sample - loss: 0.1678 - accuracy: 0.9404\n",
            "Epoch 421/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.1671 - accuracy: 0.9404\n",
            "Epoch 422/500\n",
            "453/453 [==============================] - 0s 256us/sample - loss: 0.1657 - accuracy: 0.9470\n",
            "Epoch 423/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.1649 - accuracy: 0.9404\n",
            "Epoch 424/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.1638 - accuracy: 0.9448\n",
            "Epoch 425/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.1628 - accuracy: 0.9470\n",
            "Epoch 426/500\n",
            "453/453 [==============================] - 0s 234us/sample - loss: 0.1621 - accuracy: 0.9470\n",
            "Epoch 427/500\n",
            "453/453 [==============================] - 0s 224us/sample - loss: 0.1614 - accuracy: 0.9426\n",
            "Epoch 428/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1615 - accuracy: 0.9448\n",
            "Epoch 429/500\n",
            "453/453 [==============================] - 0s 298us/sample - loss: 0.1642 - accuracy: 0.9470\n",
            "Epoch 430/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.1641 - accuracy: 0.9470\n",
            "Epoch 431/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1652 - accuracy: 0.9404\n",
            "Epoch 432/500\n",
            "453/453 [==============================] - 0s 226us/sample - loss: 0.1663 - accuracy: 0.9448\n",
            "Epoch 433/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1708 - accuracy: 0.9492\n",
            "Epoch 434/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1743 - accuracy: 0.9448\n",
            "Epoch 435/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.1663 - accuracy: 0.9448\n",
            "Epoch 436/500\n",
            "453/453 [==============================] - 0s 228us/sample - loss: 0.1667 - accuracy: 0.9426\n",
            "Epoch 437/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1589 - accuracy: 0.9448\n",
            "Epoch 438/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.1579 - accuracy: 0.9470\n",
            "Epoch 439/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1566 - accuracy: 0.9448\n",
            "Epoch 440/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1568 - accuracy: 0.9470\n",
            "Epoch 441/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1615 - accuracy: 0.9404\n",
            "Epoch 442/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.1666 - accuracy: 0.9426\n",
            "Epoch 443/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1615 - accuracy: 0.9448\n",
            "Epoch 444/500\n",
            "453/453 [==============================] - 0s 227us/sample - loss: 0.1580 - accuracy: 0.9492\n",
            "Epoch 445/500\n",
            "453/453 [==============================] - 0s 225us/sample - loss: 0.1562 - accuracy: 0.9448\n",
            "Epoch 446/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.1543 - accuracy: 0.9492\n",
            "Epoch 447/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1547 - accuracy: 0.9448\n",
            "Epoch 448/500\n",
            "453/453 [==============================] - 0s 275us/sample - loss: 0.1531 - accuracy: 0.9470\n",
            "Epoch 449/500\n",
            "453/453 [==============================] - 0s 263us/sample - loss: 0.1522 - accuracy: 0.9426\n",
            "Epoch 450/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.1520 - accuracy: 0.9470\n",
            "Epoch 451/500\n",
            "453/453 [==============================] - 0s 236us/sample - loss: 0.1512 - accuracy: 0.9492\n",
            "Epoch 452/500\n",
            "453/453 [==============================] - 0s 230us/sample - loss: 0.1504 - accuracy: 0.9470\n",
            "Epoch 453/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1494 - accuracy: 0.9448\n",
            "Epoch 454/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1483 - accuracy: 0.9448\n",
            "Epoch 455/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.1481 - accuracy: 0.9448\n",
            "Epoch 456/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.1490 - accuracy: 0.9470\n",
            "Epoch 457/500\n",
            "453/453 [==============================] - 0s 288us/sample - loss: 0.1478 - accuracy: 0.9470\n",
            "Epoch 458/500\n",
            "453/453 [==============================] - 0s 271us/sample - loss: 0.1465 - accuracy: 0.9426\n",
            "Epoch 459/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1475 - accuracy: 0.9470\n",
            "Epoch 460/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1491 - accuracy: 0.9448\n",
            "Epoch 461/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1471 - accuracy: 0.9448\n",
            "Epoch 462/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1460 - accuracy: 0.9470\n",
            "Epoch 463/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1447 - accuracy: 0.9470\n",
            "Epoch 464/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1444 - accuracy: 0.9426\n",
            "Epoch 465/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1435 - accuracy: 0.9448\n",
            "Epoch 466/500\n",
            "453/453 [==============================] - 0s 267us/sample - loss: 0.1428 - accuracy: 0.9448\n",
            "Epoch 467/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1432 - accuracy: 0.9448\n",
            "Epoch 468/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1430 - accuracy: 0.9426\n",
            "Epoch 469/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1417 - accuracy: 0.9470\n",
            "Epoch 470/500\n",
            "453/453 [==============================] - 0s 250us/sample - loss: 0.1420 - accuracy: 0.9470\n",
            "Epoch 471/500\n",
            "453/453 [==============================] - 0s 243us/sample - loss: 0.1425 - accuracy: 0.9470\n",
            "Epoch 472/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.1416 - accuracy: 0.9448\n",
            "Epoch 473/500\n",
            "453/453 [==============================] - 0s 237us/sample - loss: 0.1420 - accuracy: 0.9404\n",
            "Epoch 474/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1412 - accuracy: 0.9404\n",
            "Epoch 475/500\n",
            "453/453 [==============================] - 0s 282us/sample - loss: 0.1410 - accuracy: 0.9448\n",
            "Epoch 476/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1417 - accuracy: 0.9470\n",
            "Epoch 477/500\n",
            "453/453 [==============================] - 0s 231us/sample - loss: 0.1409 - accuracy: 0.9448\n",
            "Epoch 478/500\n",
            "453/453 [==============================] - 0s 240us/sample - loss: 0.1400 - accuracy: 0.9448\n",
            "Epoch 479/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1384 - accuracy: 0.9448\n",
            "Epoch 480/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.1417 - accuracy: 0.9448\n",
            "Epoch 481/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.1430 - accuracy: 0.9404\n",
            "Epoch 482/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1405 - accuracy: 0.9492\n",
            "Epoch 483/500\n",
            "453/453 [==============================] - 0s 261us/sample - loss: 0.1386 - accuracy: 0.9492\n",
            "Epoch 484/500\n",
            "453/453 [==============================] - 0s 302us/sample - loss: 0.1376 - accuracy: 0.9492\n",
            "Epoch 485/500\n",
            "453/453 [==============================] - 0s 264us/sample - loss: 0.1366 - accuracy: 0.9492\n",
            "Epoch 486/500\n",
            "453/453 [==============================] - 0s 253us/sample - loss: 0.1387 - accuracy: 0.9426\n",
            "Epoch 487/500\n",
            "453/453 [==============================] - 0s 246us/sample - loss: 0.1368 - accuracy: 0.9514\n",
            "Epoch 488/500\n",
            "453/453 [==============================] - 0s 239us/sample - loss: 0.1369 - accuracy: 0.9514\n",
            "Epoch 489/500\n",
            "453/453 [==============================] - 0s 238us/sample - loss: 0.1360 - accuracy: 0.9492\n",
            "Epoch 490/500\n",
            "453/453 [==============================] - 0s 241us/sample - loss: 0.1359 - accuracy: 0.9470\n",
            "Epoch 491/500\n",
            "453/453 [==============================] - 0s 235us/sample - loss: 0.1360 - accuracy: 0.9514\n",
            "Epoch 492/500\n",
            "453/453 [==============================] - 0s 244us/sample - loss: 0.1349 - accuracy: 0.9470\n",
            "Epoch 493/500\n",
            "453/453 [==============================] - 0s 258us/sample - loss: 0.1342 - accuracy: 0.9492\n",
            "Epoch 494/500\n",
            "453/453 [==============================] - 0s 248us/sample - loss: 0.1344 - accuracy: 0.9448\n",
            "Epoch 495/500\n",
            "453/453 [==============================] - 0s 249us/sample - loss: 0.1332 - accuracy: 0.9514\n",
            "Epoch 496/500\n",
            "453/453 [==============================] - 0s 245us/sample - loss: 0.1333 - accuracy: 0.9492\n",
            "Epoch 497/500\n",
            "453/453 [==============================] - 0s 247us/sample - loss: 0.1331 - accuracy: 0.9448\n",
            "Epoch 498/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.1329 - accuracy: 0.9426\n",
            "Epoch 499/500\n",
            "453/453 [==============================] - 0s 242us/sample - loss: 0.1321 - accuracy: 0.9470\n",
            "Epoch 500/500\n",
            "453/453 [==============================] - 0s 252us/sample - loss: 0.1336 - accuracy: 0.9470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YXGelKThoTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poeprYK8h-c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "869620e5-32b6-4e12-c550-dbddd80d7f4b"
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3Xl81NX97/HXJzshC2RjS9jDpiBg\nZHEXl6LW2qp1qVq1WrppvT/b/qrXXrve323tr7a1tV69P7Wt/tRaq5a6IQruiATZQSSsSUgICdn3\nZM79Y4YxYIAB8s0kM+/n45EH3++Z70w+JyTzmXPO95xjzjlEREQAYsIdgIiI9B1KCiIiEqSkICIi\nQUoKIiISpKQgIiJBSgoiIhLkWVIws0fNrMLM1h/icTOz+82syMzWmtlMr2IREZHQeNlS+DMw/zCP\nXwjkB74WAA96GIuIiITAs6TgnHsb2HeYSy4F/ur8PgAGmdkwr+IREZEjiwvj9x4BFHc5LwmUlR18\noZktwN+aYODAgSdPmjSpVwIUEYkUK1eurHTOZR/punAmhZA55x4GHgYoKChwhYWFYY5IRKR/MbOd\noVwXzruPSoG8Lue5gTIREQmTcCaFhcBXA3chzQFqnXOf6ToSEZHe41n3kZk9BZwNZJlZCfBjIB7A\nOfd/gZeBi4AioAm4yatYREQkNJ4lBefcNUd43AHf8er7i4jI0dOMZhERCVJSEBGRICUFEREJ6hfz\nFESkZzjnMDOcc7yyvpz2Th8l1c18+eRcctKScM5RUd/Kog3lfPnkPJZtqyQ7JYmctESeWVHMrDEZ\nTB85iMS42ODr1bV08PfCYuqa2wFISYojb3AyF04dxs6qRlbtquHS6cOD39fMAGjr8NHpc7y2sTwY\nX0JsDPNPHBq8BqC1o5PEuFhKa5rZtLuOMydkEx9rwWv2P35wHY/l5wLQ2NrBC6tLubIgj/jYTz83\nv7+1kriYGE4eNZg3N1ewcXcdV56SR05qIm2dvm5jcM7R1umjeF8zC9fsJi0pjhtPHU1cbPefx/dv\nj9w1/mOpz/Gw/rZHsyavSX/W3ukLvtHUNrXzxPKdTBmexsbddVw0dRgpiXFkpSTQ3N5JQ2sH2SmJ\ndPgcBsSYERPjf3PYtreBlTuruWxmLrEx3b9hVDa08vxHpVw1K4+E2Bh21zRz7X8t53sXTGRwcjw3\n/+XAv6OZIwexqriG/W8J03LTWVtS2+1rnzUhm51VjeyoajpkXf/xrblc/uAyAE4ZPZiLpg7jxbVl\nNLV1kpYUx8qd1XT4Pvv+M3dsJpOHpWEG60pr+XD7PiYNTeXj8vrgNeNzUjgzP5vXNpZTUt3MLy+b\nypC0JP64tIgte+oZmp5EXXMHy+6aF3xD9fkcnc4d8EYPsKa4hgWPFzI2K4XYGOPdokoAvnHWWAAG\nJsRx1Sl5nPubt2ho7TjguVkpCWQMTGBLRQM3njqaM/OzeeKDnazfXcv4nBTeK6rq9mfzu6umc+n0\n4Tz09jZ8zjE9bxCPvbeDxRv3APCDz03kG2eO5ZevfMyLa8u4eNow5ozN5PwpQw758z4SM1vpnCs4\n4nVKCiKH5pzj2ZUlJMXHcslJwz/z+O6aZp5eUczJowbzrzW7SUuK5zvnjCMzJRHwvxHt2tfE9spG\n7n5+HZUNbVw2cwRpA+L515rdlNW2fOY1Z43JYG1JDS3tPgBGDBpAXKwxJmsgj95wCjExxok/XhR8\ng1ryvbMYm50SfH5zWycPvlnE86tLKd7XDEBqUhwDE+Iorzvw+00amso9n5/CK+vLeXldGVWNbYD/\nTXzFjuoDrv3lZVNp7fCxeU89z39USnN7JwCXzRjB1bNGMmtMBu2dPh58cyv3Lf7kiD/b/JwULpuZ\ny566FhZtKO/2Z5GdmkhiXAwl1c0HlA9Ojqe6qf2I3+OtH5zNqMyBAPz7s2tYU1zLP289jaR4/6f6\nl9aW8Z0nPwIgLSmOupaOQ74WwHmTc9hb38qXC/KYOXIwVz+87LDPGZgQy5C0JLZVNvLdc/O5/40t\nwcem5w1idXHNIZ87ID6W5vZOEuNiaO3w/y4s+h9nMnFo6hHr3Z1Qk4K6jyTqOOdYVVzDMyuKSYiL\n4fKZuSzaUM6+xjbyh6QyLD2Jl9aWUd/aQV1zO6uLa4iLMSYMSf3MH+RDb23lL8v8qwekJMbR2tHJ\nwjWlTBmeTkKssa2ykW17Gw94ztMr/Et+5Q4ewB+umRFsEZTXtbC6uIZnV5Zw6rhMRmUm89SHxZTW\n+N8Qd1Y18aN/rufkkYMP+MT6yZ76YFLYXtnIvN+8ycGf9SYMSaWxtYO7L57MPz4qYUhqEtfMHsn0\nvEEAnDo+i59/8UQqG1rZta+JhNgYPv+Hd5l/wlDuvHAStc3tnBS4FuCnXziBtzbv5YwJWQd0m8TH\nxvDdc/Mpq23mzc17mTcph2tnjyIxPoadVY04B/Mm5bBsaxXjh6SQk5oEwE++cAIAG3bXkpwQx47K\nRk4YnkZOWhIt7Z28u6WScyblsKmsjqHpSfic46Od1cTFxPD+1ioefW873zxrHDNHDqKhtYM7nlkD\nwLKtVWSnJhJjFmyl/GHJFn7wuUls3F0XTAg/uWQKN542BoB1JbXUtbTzu9c/YfaYTPY1tfFeUSVf\nmTWSb5w17oCf642njub+JUU8cfNs/lZYTHZKIj+8cCJb9jQwICGWcdkp+HyOD7ZVMWtMBlfMzKVo\nbz1f+3NhMCFcMyuPpz4s5tdXTGPD7jpSk+LYsLuO8toWrpk9kounDuPV9eXct/gTNpbVHnNSCJVa\nChLRnHNsKqtn4tBUYmP8fby3PrWKl9aWEWPg4DNvoABxMUaHzzE0LYlrZo3kgaVFtHX6uPDEodQ0\ntRMfF4MBK3dWMzZ7IIOTE7jrokl0dDq+9d8rKd7XTGyMER9rXD9nFFsqGrh0+nA+d8JQXllXzke7\nqvn5pScGu4P26/Q5Cnfso2B0BjEGawLdN+2dPhau3s3jH+w8ID6AX10+latOGQn4Pw0/U1jCyIxk\n/n3+RMbnpJCRnEBOWtJR/+xW7NjHtNz0A970+6Lmtk42ltVx8qjBwbJlW6u46c8fBltbWSkJVDa0\nMWloKlsqGrjljDE89NY2wJ/grp8z6jP/F6Ho9DlW7qxm1piMo3re5vJ6Pve7twHY9h8XsWF3HSeO\nSDvs2EFLe2ewhXMs1H0kUe+FVaX8YckWtu5t5Iz8LC45aTiPvrudj8vrWXDmWL46dxRtHT5eWL2b\nVbuqGTFoABdOHYZzjoLRGWzf28ikYanEx8bw8xc38si72wFIHxBPbWBQdfaYDL53wcQD3hTqW9op\nr20hfUA8rR0+8jKSe6Q+tU3tnPrLNxiUnMBL3z2dP725lYff3sat54znkpOGMyozmVN/uYTx2Sk8\ndP3JDB6Y0CPft7/65uMreXVD+QFlK+4+jwt//zaVDW2Mz0nh11dMY8bIwYd4BW/ta2yjtaOTYekD\neuX7KSlIVKpqaGXJxxUU7qjmb4XFjM9JIXNgAquKa2jr8JExMIFrZuXx/QsmHtUdHR2dPh7/YCc/\n/ddGnrxlNgseX8n0vEE8cctsD2vzWRX1LaQlxQc/MU7/2WvUBPrW547NZNm2Kp5eMIc5YzN7Na6+\nqLSmmQeWFnHd7FFcdP87XDx1GA9cO5N7X/2YP725lTvOn8B3z80Pd5i9RklBokJ7p49X15dTMHow\nL60t4+G3t1FR3wrAN84cyw8+N5G42BgqG1qpamgjL2MAyQnHPpS2r7GNjIEJNLR2EB9rYe9aGX3n\nSwecXzBlCA9/9Yh/91GnvqWdxLhYEuJiqG9p549Livj2OeNJHxAf7tB6jQaaJSI55+jwORau3s2s\nMRnc8cxqVuyoDvaxj80ayIPXzWRMVgoZXbpPslISyQrcEXQ89r9mSmLf+tN56PqT+dEL67l13vhw\nh9InpSbFH3B810WTwxhN39a3frNFDqGyoZU7nllDeW0zs8dkBgdcE2JjuHT6cFYX1/AfX5rK3LGZ\nxzRg2F8tvPU06po7OD0/i/MnD4mquos3lBSkz9tcXs8Nj37IvqY22jp8fLKnAfB/av+38/K5fu7o\n8AYYRtNyP71NVAlBeoKSgvRZe+tb+fmLG4MzX5//9qnUNrVT09zO/BOG6k1QxANKCtJnPfLudl5c\nu5sThqdz14WTOGF4erhDEol4WiVV+oQHlhZx2Z/eo6jCv75NR6ePZwqLuWDKUP512+mcOj4rzBGK\nRAe1FKRXlFQ3sauqieTEuODSCvt1+hx/XFJEc3sn5933NieOSOOU0Rnsa2zjizM+u96QiHhHSUF6\nxS1/KQyucrnmngtIT/70FsFNZXXBxdUA1pfWsb60jkHJ8Zw5IbvXYxWJZkoK4innHBf89m22VDQE\nyzaV1x0w4/b9rf6lim86bTTPfVTKsrvmUdPUTtqA+OOaaCYiR09jCuKpDbvrggnhuW+fCsD3/76G\n3TXNPPHBTpxzvLq+nHHZA/nxJSew+p7zSU6IY/igAX1ugphINNBfnfSolnb/0sQ3zB1N2oB4fvav\njcTGGCvuPi84G7ikuplTf7kE8LcSPtpVw1fnjgLo1R2mROSzlBSkRy3aUM4DS7eyqayeEYMG8OGO\nffzq8qnBhPDMN+Zy5UPLgte/vM6/imU0LUwm0pcpKchx29fYxn++tpmTctP54T/WYQZLPq4A/OME\n+9f6B/+uYkPSEtlT1xosu2HuqB5Zl0hEjp+SghyXprYObn96Fe9sqeTJ5TA0LYnvzBvPJ+X1nDA8\njatOyfvMc/JzUg9ICpOHpfVmyCJyGEoKclz+tHQr72ypJC7GWHDmWL57bv4Rd4e6elYe7xZVMnlY\nGmOykpk3OaeXohWRI1FSkGPS0t7JP1eX8vyqUvJzUvjXbaeHvFXg56cN5+yJOSTHx2r9IpE+RklB\njlrxvqbgPgYA91150lHvHavbTUX6Jv1lSkg+2VPP6uIahqcP4LpHlgPwrbPHcdu88ZpgJhJB9Ncs\nIbnruXWs3FkdPP/W2eP44fxJYYxIRLygpCAhqWlqA2DqiHSuLMjlywWfvatIRPo/JQU5rKKKeu75\n5wa27m3km2eN484L1ToQiWRa+0gOa+Hq3by/tQqAglGDwxyNiHhNLQU5rFXFNUwelsYjNxQwLD0p\n3OGIiMc8bSmY2Xwz22xmRWZ2ZzePjzSzpWa2yszWmtlFXsYjoev0Of66bAcfbKtiet4ghg8aoMXq\nRKKAZ0nBzGKBB4ALgSnANWY25aDLfgQ845ybAVwN/MmreOToLN64h3v+uYHEuFhuOm10uMMRkV7i\nZffRLKDIObcNwMyeBi4FNna5xgH7F75JB3Z7GI+E6LUN5XzziZUArPxf55EYd3QT00Sk//Ky+2gE\nUNzlvCRQ1tVPgOvMrAR4GbituxcyswVmVmhmhXv37vUiVunilfX+5az/7bwJSggiUSbcdx9dA/zZ\nOZcLXAQ8bmafick597BzrsA5V5CdrT17vdTU1sGr68s5d1IOt5+nPQ5Eoo2XSaEU6DrDKTdQ1tXN\nwDMAzrllQBKQ5WFMcgR3P7+e5vZOTh6t209FopGXSWEFkG9mY8wsAf9A8sKDrtkFnAtgZpPxJwX1\nD4XR8m1VjMkayNdOGxPuUEQkDDxLCs65DuBWYBGwCf9dRhvM7Gdm9oXAZd8Dvm5ma4CngBudc86r\nmOTwKupa2F3bwnVzRh31qqciEhk8nbzmnHsZ/wBy17J7uhxvBE7zMgYJ3f4tNDVzWSR6hXugWXpZ\n14ZY1+MX1+7mzufWMXlYGtNy08MRmoj0AUoKUaR4XxMn/fQ1Fm0o55V1Zcz4+WJeXOufGvJekX99\no3svn6aZyyJRTGsfRYl1JbXc+NiH1LV08I3HVzIyI5mapnZ+9MJ65ozNZEdlIzNHDmKqWgkiUU0t\nhSjQ2tHJJX98l6rGtmDZrn1NfGX2SBpaOvivd7azo6qR0ZkDwxiliPQFSgpRYFdVEwCZAxO4bOan\nk8pvPHU0IzOT2bKnnrLaFkZnKSmIRDt1H0WBHYGk8MiNpzBlWBrPfVTKSbnpTBiSSlZKIu9sqQRg\nwpCUcIYpIn2AkkIU2FHZCMCYzIEkxMXw/p3zyEpJBCA7NZG2Th8AM0bqVlSRaKekEAW2VzUyKDme\n9OR4AIYPGhB8LDuQHIalJzEkTZvoiEQ7jSlEgc3l9UzISe32sexUf1KYOkJ3HYmIkkLE8/kcm8rq\nmDys+6RQ29wOwIQh3T8uItFFSSHC7drXRFNbJ1OGp3X7+BUn5zI8PYlrZo/s5chEpC/SmEKEe3l9\nGQAFozO6fXzCkFTev+vc3gxJRPowtRQi3LOFJcwdm8m4bN1uKiJHpqQQwepb2tlW2cip4zLDHYqI\n9BPqPopQpTXNvLvFv1/RocYTREQOpqQQoc741RJ8gZWxJw9TUhCR0Kj7KELtTwgJcTEMS9ekNBEJ\njZJCBOq6ec7AhFjtjyAiIVNSiEB761uDx7+9anoYIxGR/kZJIQIVV/tXRX3sxlM4e2JOmKMRkf5E\nA80R5LmPSmjv9NHU1glAvpbCFpGjpKQQQe54Zg0A508ZwsiMZHIHJ4c5IhHpb9R9FAHaOnx8/a+F\nwfMlH1cwZ2z3y1qIiByOkkIEeOy97SzeuCd43ulzWgpbRI6JkkI/19bh45F3t3+mXLOYReRYaEyh\nn1tTUkNFfSv3XjGN1g4f/7loM7XN7UwcqqQgIkdPSaEfc87x4fZ9AJyRn8Ww9AFcMGUI60trSUnU\nf62IHD29c/RjC9fs5teLNgMwNLC/8pA07bUsIsdOYwr92Gsb/IPL+TkpWspCRHqEkkI/9cq6Ml5a\nV8bY7IE8+fU54Q5HRCKEkkI/9ZvFnwDwlVkjyU5NDHM0IhIplBT6ofqWdrZXNnLjqaO56bQx4Q5H\nRCKIkkI/tK6klk6fY96kHGJjNJYgIj3H06RgZvPNbLOZFZnZnYe45koz22hmG8zsSS/jiRTbKhsB\nGJejBe9EpGd5dkuqmcUCDwDnAyXACjNb6Jzb2OWafOAu4DTnXLWZaZ3nEOysaiQxLoZhuvVURHqY\nly2FWUCRc26bc64NeBq49KBrvg484JyrBnDOVXgYT8TYXtnEqMxkYtR1JCI9zMukMAIo7nJeEijr\nagIwwczeM7MPzGy+h/FEhLqWdlYXVzMuW11HItLzwj2jOQ7IB84GcoG3zWyqc66m60VmtgBYADBy\n5MjejrFPeWr5Liob2vjmWePCHYqIRCAvWwqlQF6X89xAWVclwELnXLtzbjvwCf4kcQDn3MPOuQLn\nXEF2drZnAfcHy7fvY1z2QE7KGxTuUEQkAnmZFFYA+WY2xswSgKuBhQdd8wL+VgJmloW/O2mbhzH1\naz6fo3DHPk4ZrQ10RMQbISUFM3vOzC42s5CTiHOuA7gVWARsAp5xzm0ws5+Z2RcCly0CqsxsI7AU\n+IFzruroqhAd2jp8/OKlTdS1dCgpiIhnQh1T+BNwE3C/mf0deMw5t/lIT3LOvQy8fFDZPV2OHXBH\n4EsO48W1u3n0Pf9mOkoKIuKVkD75O+ded85dC8wEdgCvm9n7ZnaTmcV7GaD4VdS3Bo/zMgaEMRIR\niWQhdweZWSZwI3ALsAr4Pf4ksdiTyOQAOwKzmF+87XQtky0ingmp+8jMngcmAo8DlzjnygIP/c3M\nCr0KTj61vbKRk0cN5sQR6eEORUQiWKhjCvc755Z294BzrqAH45Fu7KlrYX1pLZ+fNjzcoYhIhAu1\n+2iKmQVvjDezwWb2bY9ikoM8uXwXze2dfPNsTVgTEW+FmhS+3nWWcWCtoq97E5IcbOveBvIykhmT\nNTDcoYhIhAs1KcRal9HNwAqoCd6EJAfbUdXIqEwlBBHxXqhJ4VX8g8rnmtm5wFOBMvGYc44dlU2M\nyUwOdygiEgVCHWj+IfAN4FuB88XAf3kSkQQ1tXXwyrpyGlo7tKGOiPSKkJKCc84HPBj4kl7gnKPg\nF6/T1NbJtNx0vjTj4FXHRUR6XqjzFPKB/wNMAYLbfTnnxnoUV9Qrq22hqa0TgB/On0RqkiaOi4j3\nQh1TeAx/K6EDOAf4K/CEV0EJbCqrA+DeK6Zx2visMEcjItEi1KQwwDn3BmDOuZ3OuZ8AF3sXlmzY\n7U8KF00dFuZIRCSahDrQ3BpYNnuLmd2Kf7McjXx6aOnmCk4YnkZKYrg3xxORaBJqS+F2IBn4LnAy\ncB1wg1dBRbvy2hZW7arhwhOHhjsUEYkyR/wYGpiodpVz7vtAA/59FcRDizaUAzD/RHUdiUjvOmJL\nwTnXCZzeC7EIsLummQff3Ep+TgrjNTdBRHpZqB3Wq8xsIfB3oHF/oXPuOU+iimKPf7CTPfUtPHDt\n3HCHIiJRKNSkkARUAfO6lDlASaGHvbq+nNPHZ3HyKG25KSK9L9QZzRpH6AXltS1sr2zkujmjwh2K\niESpUGc0P4a/ZXAA59zXejyiKLa6uBqAGSMHHeFKERFvhNp99GKX4yTgS8Dung8nerW0d/LsylLi\nY40pw9LCHY6IRKlQu4/+0fXczJ4C3vUkoij1vb+v4fVNe/j+BRNIio8NdzgiEqVCnbx2sHwgpycD\niXYfbt/HZTNGcOu8/HCHIiJRLNQxhXoOHFMox7/HgvSA1o5O9ta3anc1EQm7ULuPUr0OJJqV17YA\nMHxQ0hGuFBHxVkjdR2b2JTNL73I+yMy+6F1Y0aW0phmAEYMHhDkSEYl2oY4p/Ng5V7v/xDlXA/zY\nm5CiT0l1ICkMUlIQkfAKNSl0d53WdO4hy7ZWkT4gXklBRMIu1KRQaGb3mdm4wNd9wEovA4sWHZ0+\nXt+0h/OnDCEu9lhvBhMR6RmhvgvdBrQBfwOeBlqA73gVVLRo7/Tx8xc3Ut/SwRn52nJTRMIv1LuP\nGoE7PY4l6ry6vpy/LNsJwMyRg8McjYhI6HcfLTazQV3OB5vZIu/Cig77B5gBcnXnkYj0AaF2H2UF\n7jgCwDlXjWY0H7ctFfUALLz1NMwszNGIiISeFHxmNnL/iZmNpptVUw9mZvPNbLOZFZnZIbufzOxy\nM3NmVhBiPP3e2pIanvuolNPGZzItV6uiikjfEOptpXcD75rZW4ABZwALDveEwN7ODwDnAyXACjNb\n6JzbeNB1qcDtwPKjjL1fe2ltGQBXFuSFORIRkU+F1FJwzr0KFACbgaeA7wHNh30SzAKKnHPbnHNt\n+O9aurSb634O/Ar/HU1Ro6SmmTFZA7l0+ohwhyIiEhTqQPMtwBv4k8H3gceBnxzhaSOA4i7nJYGy\nrq87E8hzzr10hO+/wMwKzaxw7969oYTc55VUN2twWUT6nFDHFG4HTgF2OufOAWYANYd/yuGZWQxw\nH/5Ec1jOuYedcwXOuYLs7Ozj+bZ9Rml1s2Ywi0ifE2pSaHHOtQCYWaJz7mNg4hGeUwp07TDPDZTt\nlwqcCLxpZjuAOcDCaBhsbmnvpLKhVS0FEelzQh1oLgnMU3gBWGxm1cDOIzxnBZBvZmPwJ4Orga/s\nfzCwwF5wGq+ZvQl83zlXGHr4/VNwATwlBRHpY0Kd0fylwOFPzGwpkA68eoTndJjZrcAiIBZ41Dm3\nwcx+BhQ65xYeR9z92v6lsnMHJ4c5EhGRAx31SqfOubeO4tqXgZcPKrvnENeefbSx9Ed761u54dEP\nAS2VLSJ9j5bl7GVPLt8VPB6Spp3WRKRvUVLoRZUNrazcVR08j43R0hYi0rdoo5xe0tjaQcEvXg+e\n/+6q6WGMRkSke2op9ILmtk5+/8aWA8q+OEMzmUWk71FLoRf875c38sQHn44l/OBzR5riISISHkoK\nvWBHZVPw+LV/O5MJQ1LDGI2IyKGp+6gXdPh8weNByfFhjERE5PCUFHpBR+enW08MTk4IYyQiIoen\npOCh+pZ2rn9kOYU7P70NNT5WP3IR6bv0DuWhwp3VvLOlMtxhiIiETEnBQ5+U+/dgXnDm2DBHIiIS\nGt195KHNe+oZmpbE/7xoMmdNyKbTd8RtrUVEwkpJwUOby+uZMNR/++lp47OOcLWISPip+8gjnT7H\nlooGJg5JCXcoIiIhU1LwyM6qRto6fJqoJiL9ipKCRzYHBpknDU0LcyQiIqFTUvDIO0WVJCfEkq/u\nIxHpR5QUPODzOV7bsIdzJuWQFB8b7nBEREKmpOCBHVWNVDa0cobuOBKRfkZJwQPrSmsBmJqbHuZI\nRESOjpKCB9aX1pIQF6M7j0Sk31FS8MC60lomD03V4nci0u/oXauH+XyODaV1nDhCXUci0v8oKfSw\nbZUN1Ld2MFVJQUT6ISWFHuSc495XN5MYF8Pp+brzSET6HyWFHvTm5r28tnEPd5w/gdzByeEOR0Tk\nqCkp9KBVxTWYwddOHxPuUEREjomSQg+qamglIzlBdx2JSL+ld68esq6kludXlZKZkhDuUEREjpk2\n2ekhl/zxXQAyByaGORIRkWOnlkIPS4rXj1RE+i+9g/WAupb24HFNc/thrhQR6duUFHrA0o8rgsed\nPhfGSEREjo/GFI6Tc45fvLSJ4elJnJGfzS1n6HZUEem/PG0pmNl8M9tsZkVmdmc3j99hZhvNbK2Z\nvWFmo7yMxwu79jWxt76VW+fl86srppGvlVFFpB/zLCmYWSzwAHAhMAW4xsymHHTZKqDAOTcNeBa4\n16t4vLJqVw0A0/MGhTkSEZHj52VLYRZQ5Jzb5pxrA54GLu16gXNuqXOuKXD6AZDrYTyeWF1cw4D4\nWCZoL2YRiQBeJoURQHGX85JA2aHcDLzS3QNmtsDMCs2scO/evT0Y4vFbVVzDtNx04jSLWUQiQJ94\nJzOz64AC4NfdPe6ce9g5V+CcK8jOzu7d4A6jpb2TjbtrmT5SXUciEhm8vPuoFMjrcp4bKDuAmZ0H\n3A2c5Zxr9TCeHrexrI72TscMjSeISITwsqWwAsg3szFmlgBcDSzseoGZzQAeAr7gnKvo5jX6LOcc\nTy7fBcD0vMFhjkZEpGd4lhSccx3ArcAiYBPwjHNug5n9zMy+ELjs10AK8HczW21mCw/xcn3O4o17\neHZlCQBD05PCHI2ISM/wdPKac+5l4OWDyu7pcnyel9/fS1sqGgC494ppYY5ERKTn9ImB5v6oeF8T\nWSkJXFmQd+SLRUT6CSWFY1Tv0YeuAAAJM0lEQVRc3aQtN0Uk4igpHKW2Dh+/e/0TCndUk5ehpCAi\nkUUL4h2lJR9X8LvXtwAwU/MTRCTCqKVwlF5dXwbAxdOGcd2cfrd+n4jIYamlcBT2NbbxyvpyvjJ7\nJP/xpanhDkdEpMeppXAUFm0op7XDx/VqIYhIhFJSOAofl9WRkhjHpKHaM0FEIpOSwlHYvKeeCUNS\nMLNwhyIi4gklhRA559hcXs9EtRJEJIIpKYRob0Mr1U3tTNB2myISwZQUQvRJuX+to4lKCiISwZQU\nQlDX0s51jywHUPeRiEQ0zVM4jKqGVhpbO3mnyL8FaGyMkZmSGOaoRES8o6RwCO2dPub+cgltHT5m\njc5gSFoi/7rt9HCHJSLiKXUfdeONTXvIv/sV2jp8AHy4Yx8LzhxHTqo20xGRyKaWQjf+sKQoeJyT\nmsgpozO48dTR4QtIRKSXKCkcZOnHFawurmFwcjxPL5hLfk4KMTGarCYi0UFJoYvCHfu46c8riIsx\nfnvVdN1pJCJRR0mhi+Xb9wHw4d3nkTEwIczRiIj0Pg00d7Fxdx15GQOUEEQkaikpAE1tHdz85xUs\n3VzB5KFp4Q5HRCRslBSA94uqeOPjCk4Ynsa12itBRKKYxhSAd4sqSYyL4YlbZpMYFxvucEREwiaq\nksLKndUkxMYwZXgar6wv4zevfcKcsZn8bcUuzp08RAlBRKJeVCWFyx98/4DzGIPtlY1cPG0Yv7p8\nWpiiEhHpO6ImKfh8DoDEuBi+c854MgYmMHtMBlsqGrjwxKHaTU1EhChKCpWNrQD86OLJXD93dLA8\nX/sjiIgERc3dR+W1LQAMTR8Q5khERPqu6EsKaVrpVETkUKInKdTtbykoKYiIHErUJIWhaUlcMGUI\nmVrCQkTkkKJmoPmCE4ZywQlDwx2GiEif5mlLwczmm9lmMysyszu7eTzRzP4WeHy5mY32Mh4RETk8\nz5KCmcUCDwAXAlOAa8xsykGX3QxUO+fGA78FfuVVPCIicmRethRmAUXOuW3OuTbgaeDSg665FPhL\n4PhZ4FzTLDIRkbDxMimMAIq7nJcEyrq9xjnXAdQCmR7GJCIih9Ev7j4yswVmVmhmhXv37g13OCIi\nEcvLpFAK5HU5zw2UdXuNmcUB6UDVwS/knHvYOVfgnCvIzs72KFwREfEyKawA8s1sjJklAFcDCw+6\nZiFwQ+D4CmCJc855GJOIiByGZ/MUnHMdZnYrsAiIBR51zm0ws58Bhc65hcAjwONmVgTsw584REQk\nTKy/fTA3s73AzmN8ehZQ2YPh9Aeqc3RQnaPD8dR5lHPuiP3v/S4pHA8zK3TOFYQ7jt6kOkcH1Tk6\n9Ead+8XdRyIi0juUFEREJCjaksLD4Q4gDFTn6KA6RwfP6xxVYwoiInJ40dZSEBGRw1BSEBGRoKhJ\nCkfa26G/MrNHzazCzNZ3Kcsws8VmtiXw7+BAuZnZ/YGfwVozmxm+yI+dmeWZ2VIz22hmG8zs9kB5\nxNbbzJLM7EMzWxOo808D5WMCe5EUBfYmSQiUR8ReJWYWa2arzOzFwHlE1xfAzHaY2TozW21mhYGy\nXvvdjoqkEOLeDv3Vn4H5B5XdCbzhnMsH3gicg7/++YGvBcCDvRRjT+sAvuecmwLMAb4T+P+M5Hq3\nAvOccycB04H5ZjYH/x4kvw3sSVKNf48SiJy9Sm4HNnU5j/T67neOc256lzkJvfe77ZyL+C9gLrCo\ny/ldwF3hjqsH6zcaWN/lfDMwLHA8DNgcOH4IuKa76/rzF/BP4PxoqTeQDHwEzMY/uzUuUB78Pce/\nvMzcwHFc4DoLd+xHWc/cwBvgPOBFwCK5vl3qvQPIOqis1363o6KlQGh7O0SSIc65ssBxOTAkcBxx\nP4dAN8EMYDkRXu9AV8pqoAJYDGwFapx/LxI4sF6RsFfJ74B/B3yB80wiu777OeA1M1tpZgsCZb32\nu+3ZgnjSNzjnnJlF5H3HZpYC/AP4H865uq6b9kVivZ1zncB0MxsEPA9MCnNInjGzzwMVzrmVZnZ2\nuOPpZac750rNLAdYbGYfd33Q69/taGkphLK3QyTZY2bDAAL/VgTKI+bnYGbx+BPCfzvnngsUR3y9\nAZxzNcBS/N0ngwJ7kcCB9Qppr5I+7DTgC2a2A/9WvvOA3xO59Q1yzpUG/q3An/xn0Yu/29GSFELZ\n2yGSdN2n4gb8fe77y78auGNhDlDbpUnab5i/SfAIsMk5d1+XhyK23maWHWghYGYD8I+hbMKfHK4I\nXHZwnfvtXiXOubucc7nOudH4/16XOOeuJULru5+ZDTSz1P3HwAXAenrzdzvcgyq9OHhzEfAJ/n7Y\nu8MdTw/W6ymgDGjH3594M/6+1DeALcDrQEbgWsN/F9ZWYB1QEO74j7HOp+Pvd10LrA58XRTJ9Qam\nAasCdV4P3BMoHwt8CBQBfwcSA+VJgfOiwONjw12H46j72cCL0VDfQP3WBL427H+v6s3fbS1zISIi\nQdHSfSQiIiFQUhARkSAlBRERCVJSEBGRICUFEREJUlIQCTCzzsDKlPu/emw1XTMbbV1WshXpq7TM\nhcinmp1z08MdhEg4qaUgcgSB9e3vDaxx/6GZjQ+UjzazJYF17N8ws5GB8iFm9nxg74M1ZnZq4KVi\nzez/BfZDeC0wMxkz+67594ZYa2ZPh6maIoCSgkhXAw7qPrqqy2O1zrmpwB/xr94J8AfgL865acB/\nA/cHyu8H3nL+vQ9m4p+ZCv417x9wzp0A1ACXB8rvBGYEXuebXlVOJBSa0SwSYGYNzrmUbsp34N/g\nZltgIb5y51ymmVXiX7u+PVBe5pzLMrO9QK5zrrXLa4wGFjv/JimY2Q+BeOfcL8zsVaABeAF4wTnX\n4HFVRQ5JLQWR0LhDHB+N1i7HnXw6pncx/vVrZgIruqwCKtLrlBREQnNVl3+XBY7fx7+CJ8C1wDuB\n4zeAb0FwY5z0Q72omcUAec65pcAP8S/5/JnWikhv0ScSkU8NCOxstt+rzrn9t6UONrO1+D/tXxMo\nuw14zMx+AOwFbgqU3w48bGY3428RfAv/SrbdiQWeCCQOA+53/v0SRMJCYwoiRxAYUyhwzlWGOxYR\nr6n7SEREgtRSEBGRILUUREQkSElBRESClBRERCRISUFERIKUFEREJOj/A4NHL30Tp35BAAAAAElF\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vc6PHgxa6Hm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "693f4673-a7ec-40eb-9272-60343d14ea9a"
      },
      "source": [
        "seed_text = \"Laurence went to dublin\"\n",
        "next_words = 100\n",
        "\n",
        "# Use the seed sequence and then concatinate the predicted workd \n",
        "# and with that word added  feed the predictor over and over\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Laurence went to dublin that died and wine and ladies ladies was ask ask eyes all all all me me me odaly them hearty all hearty in ladies wine for hearty hearty lanigans ball ball ball was ask ask eyes me me odaly odaly them me me a plenty time the nonsense painted tea ladies boys nelly mad at me me me me a sounded a sounded her then a reel in again all mad brooks taras old hall hall hall glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten glisten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKGq-z4j_6MF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}